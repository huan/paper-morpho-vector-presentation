# paper-morpho-vector-presentation

[![Open Paper Project](https://img.shields.io/badge/üìù-Open%20Paper%20Project-brightgreen.svg)](https://github.com/BUPT/open-paper-project)

A Mini Review of Word Embedding in Morpho

![Morpho Butterfly](https://huan.github.io/paper-morpho-vector-presentation/images/morpho-logo.png)
> picture credit: [@juditfigarolacoach](https://juditfigarolacoach.wordpress.com/2017/05/19/a-puerto/)

| morpho |
| ------ |
| Ëã± ['m…îÀêf…ô ä] Áæé ['m…îÀêfo ä] |
| n. [ÊòÜ]Â§ßÈó™Ëù∂( ‰∫ß‰∫éÂçóÁæéÊ¥≤) |
| pref. Ë°®Á§∫‚ÄúÂΩ¢ÂºèÔºõÂΩ¢ÊÄÅÔºõÁªìÊûÑÔºõËØ≠Á¥†‚Äù |

name as textmeme? meme2vec?

## USAGE

This is a [Open Paper Project](https://github.com/BUPT/open-paper-project), which means:

1. You can join if you are interested
1. PR is welcome to contribute
1. All contribtors who's PR had been accepted, will be listed as the authors in this paper.

> This project is my first paper, which the main propuse of it is to learn how to write paper for the first time. ;-D

## INSTALL

```shell
brew install pandoc pandoc-filter pandoc-citeproc
npm install -g nodemon
```

## BUILD

```shell
make clean
make pdf
```

The paper will be generated and save to `dist/A Mini Review for Word Embedding in Morpho.pdf`.

## HISTORY

### master

- TBW

### v0.0.1 Oct 2018

- [A Mini Review for Word Embeddingin Morpho.pdf](https://github.com/huan/paper-morpho-vector-presentation/releases/download/v0.0.1/A_Mini_Review_for_Word_Embedding_in_Morpho.pdf)


## SEE ALSO

- [How to make a scientific looking PDF from markdown (with bibliography)](https://gist.github.com/maxogden/97190db73ac19fc6c1d9beee1a6e4fc8)
- [plain text, papers, pandoc](https://kieranhealy.org/blog/archives/2014/01/23/plain-text/)
- [How to write an ACM-styled conference paper using Markdown/Pandoc](https://ineed.coffee/4008/how-to-write-an-acm-styled-conference-paper-using-markdownpandoc/)
- [Defination Morpho(pho)nology](http://www.ello.uos.de/field.php/EarlyModernEnglish/DefinitionMorphonology)

## PAPERS

> from @wangshirui33

1. „Ääsubword language modeling with neural networks„Äã
‰ª•ÂÖÉÈü≥‰∏∫Èü≥ËäÇÊãÜÂàÜÂçïËØç‰∏∫subwordÔºåÈÄâÊã©s‰∏™Âá∫Áé∞È¢ëÁéáÊúÄÈ´òÁöÑÈü≥ËäÇÔºå‰Ωú‰∏∫‰ªäÂêéÊãÜÂàÜÂçïËØçÁöÑ‰æùÊçÆÔºåÈááÁî®NNLMÁöÑÊñπÂºèËÆ≠ÁªÉ

2. „ÄäTokenization-free pre-trained subword embeddings in 275 languages„Äã
Âü∫‰∫éBPE(ÂèåÂ≠óËäÇÁºñÁ†ÅÔºâÊñπÊ≥ïÂíåÁª¥Âü∫ÁôæÁßëËØ≠ÊñôÈ¢ÑËÆ≠ÁªÉ‰∫Ü275ÁßçËØ≠Ë®ÄÁöÑËØçÂêëÈáèÈõÜÂêà

3. „Äänerual machine translation of rare words with subword units„Äã
ÊèêÂá∫‰∫ÜBPEÔºàÂèåÂ≠óËäÇÁºñÁ†ÅÔºâÊñπÊ≥ï
Ê†πÊçÆËØ≠ÊñôÊûÑÈÄ†ËØçÂÖ∏ÔºåÂÖàÂàáÂàÜÊàêÂçï‰∏™Â≠óÁ¨¶ÔºåÈ¢ëÁéáÊúÄÈ´òÁöÑÁõ∏ÈÇªÂÖÉÁ¥†ÁªìÂêàÊàêsubwordÔºå‰ª•Ê≠§Á±ªÊé®„ÄÇ

4. „ÄäImplicitly Incorporating Morphological Information into Word Embedding„Äã
Â∞ÜËÉΩÂ§ü‰ª£Ë°®ËØçÊ†π„ÄÅÂâçÁºÄÂíåÂêéÁºÄÁöÑÂê´‰πâÁöÑËØç‰Ωú‰∏∫Áã¨Á´ãtokenÂèÇ‰∏éÂª∫Ê®°(CBOWÊ®°Âûã)Ôºà‰πãÂâçÈÉΩÊòØÂ∞ÜÂçïËØçÂàáÂàÜ‰∏∫characterÊàñsubwordÔºåÂà©Áî®Ëøô‰∫õÂ≠êÂ≠óÁ¨¶Êù•Âª∫Ê®°ÔºåÊ≤°ÊúâËÄÉËôëÂà∞ÂÖ∂ËØçÁºÄÁöÑÈÄöÁî®Âê´‰πâÔºâ

5. „ÄäCharacter-Aware Neural Language Models„Äã
Â∞ÜÂçïËØçÁöÑcharacterÂêëÈáèÁü©ÈòµÂà©Áî®CNN\ÊúÄÂ§ßÊ±†ÂåñÂæóÂà∞‰∏Ä‰∏™Ë°®Á§∫ÂêëÈáèÔºå‰Ωú‰∏∫LSTMÁöÑËæìÂÖ•È¢ÑÊµã‰∏ã‰∏Ä‰∏™ÂçïËØç„ÄÇ

6. „ÄäBetter Word Representations with Recursive Neural Networks for Morphology„Äã
Â∞ÜËØçÊãÜÂàÜ‰∏∫ÂâçÁºÄ„ÄÅËØçÊ†π„ÄÅÂêéÁºÄÔºåÂà©Áî®RNNËÆ≠ÁªÉÂÖ∂ËØçÁºÄÂæóÂà∞subword representationÔºåÁà∂ËØçÁöÑembeddingÊòØÁî±ÊâÄÊúâËØçÁ¥†ÂæóÂà∞ÁöÑÔºåÁªìÂêàÁ•ûÁªèËØ≠Ë®ÄÊ®°ÂûãÔºàÂà©Áî®‰∏ä‰∏ãÊñá‰ø°ÊÅØÈ¢ÑÊµãÊüê‰∏ÄÂçïËØçÔºâÊù•ËÆ≠ÁªÉÊîπÂñÑÂçïÁ∫ØÁî±ËØ≠Á¥†RNNÂæóÂà∞ÁöÑembedding„ÄÇ

7. „ÄäEnriching Word Vectors with Subword Information„Äã
ÈááÁî®word2vecÁöÑskip-gramÊ®°ÂûãÔºå‰ΩøÁî®Â≠óÊØçn-gram‰Ωú‰∏∫Âçï‰ΩçÔºåÊú¨ÊñánÂèñÂÄº‰∏∫3~6Ôºå‰∏Ä‰∏™ËØçÁöÑembeddingË°®Á§∫‰∏∫ÂÖ∂ÊâÄÊúân-gramÁöÑÂíåÔºåÁî®‰∏≠ÂøÉËØçÁöÑn-gram embeddingÈ¢ÑÊµãÁõÆÊ†áËØçÊù•ËÆ≠ÁªÉ„ÄÇ

8. „ÄäJoint Learning of Character and Word Embeddings„Äã
ÔºàCWEÊ®°ÂûãÔºâÈááÁî®word2vecÁöÑcbowÊ®°ÂûãÔºåÂú®Ê®°ÂûãÁöÑËæìÂÖ•Â±Ç‰∏≠ÔºåÂºïÂÖ•‰∫ÜËØçËØ≠ÁªÑÊàêÊàêÂàÜÁöÑÂçï‰∏™Ê±âÂ≠óÁöÑ‰ø°ÊÅØÔºàËÆ∫Êñá‰∏ªË¶ÅÈíàÂØπÁöÑÊòØ‰∏≠ÊñáÔºâÔºåÊèêÂçá‰∫ÜËØçÂêëÈáèÁîüÊàêÁöÑË¥®Èáè„ÄÇÂ¶Ç Êô∫ËÉΩ+Âà∞Êù• --> Êó∂‰ª£ ÔºåÊô∫ËÉΩË¢´Ë°®Á§∫‰∏∫Êô∫ÂíåËÉΩ‰∏§‰∏™representationÁöÑÂíå„ÄÇ

9. „ÄäBag of Tricks for Efficient Text Classification„Äã
fasttextÊ®°ÂûãÔºåÈááÁî®cbowÊ®°ÂûãÔºå‰∏çÂêå‰πãÂ§ÑÂú®‰∫écbowÈ¢ÑÊµãÁõÆÊ†áËØçÔºåfasttextÈ¢ÑÊµãÊ†áÁ≠æÔºå‰∏îËæìÂÖ•Â±ÇÁªìÂêà‰∫ÜËØçËØ≠ÁöÑn gramËØ≠Á¥†‰ø°ÊÅØ„ÄÇÂ¶Çgoogle--> go,goo,oog,ogl,gle,le  Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊØè‰∏™n-gramÈÉΩ‰ºöÂØπÂ∫îËÆ≠ÁªÉ‰∏Ä‰∏™ÂêëÈáèÔºåËÄåÂéüÊù•ÂÆåÊï¥ÂçïËØçÁöÑËØçÂêëÈáèÂ∞±Áî±ÂÆÉÂØπÂ∫îÁöÑÊâÄÊúân-gramÁöÑÂêëÈáèÊ±ÇÂíåÂæóÂà∞„ÄÇ

10. „ÄäA Joint Model for Word Embedding and Word Morphology„Äã
ÊèêÂá∫‰∫Ü‰∏ÄÁßçchar2vecÊ®°ÂûãÔºåÂÖàÂà©Áî®Bi-LSTMËé∑ÂèñËæìÂÖ•Â∫èÂàóÔºàÂçïËØçÔºâÁöÑÊØè‰∏™‰ΩçÁΩÆÁöÑË°®Á§∫ÔºåÂâçÂêëlstmÂèØËé∑ÂæóÂâçÁºÄÂíåËØçÊ†πÁöÑË°®Á§∫ÔºåÂèçÂêëlstmÂèØËé∑ÂèñÂêéÁºÄÂèäËØçÊ†πÁöÑË°®Á§∫ÔºåÂ∞Ü‰∏§‰∏™hiÁªìÂêàÂç≥ÂæóÂà∞ÊØè‰∏™‰ΩçÁΩÆÁöÑË°®Á§∫ÔºåÊúâ‰∫ÜÂ≠óÁ¨¶ÁöÑË°®Á§∫ÔºåÊé•‰∏ãÊù•Áî®attentionÊú∫Âà∂Êù•ÊûÑÈÄ†ËØçÁöÑË°®Á§∫ÔºåÂ≠¶‰π†ËØ•ËØçÁöÑËØ≠‰πâ‰∏éÂì™‰∏ÄÈÉ®ÂàÜÂÖ≥Á≥ªÊõ¥Â§ßÔºåÂæóÂà∞ËØçÂêëÈáèÔºåÊúÄÂêéÂà©Áî®skip-gramÊ®°ÂûãËøõË°åËÆ≠ÁªÉÊîπÂñÑËØ≠‰πâÁõ∏ÂÖ≥ÊÄß„ÄÇ

11. „ÄäImprove Chinese Word Embeddings by Exploiting Internal Structure„Äã
Âü∫‰∫é8ËÆ∫ÊñáÁöÑCWEÊ®°Âûã„ÄÇËôΩÁÑ∂CWEÂ∑≤ÁªèËÄÉËôë‰∫ÜËØçÁöÑÂÜÖÈÉ®ÁªÑÊàêÔºåÂ¢ûÂä†‰∫ÜËØ≠‰πâ‰ø°ÊÅØÁöÑË°®Á§∫Ôºå‰ΩÜÂú®ÊØè‰∏Ä‰∏™ËØçÂíå‰ªñ‰ª¨ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºàÂçïÂ≠óÔºâ‰πãÈó¥ÔºåCWEÊääÊØè‰∏™Â≠óÁöÑË¥°ÁåÆÊòØËßÜ‰∏∫‰∏ÄÊ†∑ÁöÑ„ÄÇÊú¨ÊñáË¶ÅÂà©Áî®Â§ñÈÉ®ËØ≠Ë®ÄÊù•Ëé∑ÂèñËØ≠‰πâ‰ø°ÊÅØÔºåËÆ°ÁÆóËØç‰∏éÂçïÂ≠ó‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶Êù•‰ΩìÁé∞ÂÖ∂Ë¥°ÁåÆÂ∫¶Â∑ÆÂºÇ„ÄÇÔºà‰∏≠ÊñáËØ≠ÊñôÁøªËØëÊàêËã±ÊñáËØ≠ÊñôÔºåÂà©Áî®cbowËÆ≠ÁªÉËã±ÊñáËØ≠ÊñôÂæóÂà∞Ëã±ÊñáÂçïËØçÁöÑËØçÂêëÈáèÔºåÈÄöËøáËøô‰ªΩËØçÂêëÈáèÊú∫ÈÖ∏ËØç‰∏éÂçïÂ≠ó‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÔºâÔºåÂú®ËØçËØ≠Áõ∏‰ººÊÄßÂíåÊñáÊú¨ÂàÜÁ±ª‰∏äÊúâÂæàÂ•ΩÁöÑÊïàÊûú„ÄÇ

12. „ÄäMorphological Word-Embeddings„Äã
‰ª•Â§ö‰ªªÂä°ÁõÆÊ†áÂ¢ûÂº∫‰∫Ü mnih Âíå hinton (2007Âπ¥) ÁöÑÂØπÊï∞ÂèåÁ∫øÊÄßÊ®°Âûã (LBL)ÔºåÈô§‰∫ÜÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™ÂçïËØçÂ§ñÔºåÂ¢ûÂä†‰∫ÜÈ¢ÑÊµãÂÖ∂ËØ≠Á¥†Ê†áÁ≠æÁöÑ‰ªªÂä°„ÄÇÂ¶ÇÔºöcityÁöÑmorphological tagsÔºöNÔºåNOMÔºåSGÔºåFEM„ÄÇÂÖ∂‰∏≠FEMÂåÖÂê´‰∫ÜÂÆÉÊòØ‰∏Ä‰∏™ÂêçËØçÔºå‰∏îÊòØÂçïÊï∞ÁöÑÂê´‰πâÔºåÊØè‰∏™ÂçïËØçtagÈÉΩÁî±Â§ö‰∏™tagsÁªÑÊàê„ÄÇ

13. „ÄäCompositional Morphology for Word Representations and Language Modelling„Äã
Ê∫êËá™LBLÊ®°ÂûãÔºå‰ª•ËØ•ÂçïËØçÁöÑËØçÁ¥†‰ø°ÊÅØ‰∏éÂçïËØçÁöÑÁªìÂêà‰Ωú‰∏∫ËØçÂêëÈáèËæìÂÖ•Ê®°ÂûãÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™ÂçïËØç„ÄÇÂ¶Çnovou: novou+novo+u

14. „Ääcw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information„Äã
Êú¨ÁØáËÆ∫ÊñáÈááÁî®Á¨îÁîª‰ø°ÊÅØ‰Ωú‰∏∫ÁâπÂæÅÔºåÁî±‰∫éÊØè‰∏™Â≠óÁ¨¶ÂåÖÂê´ÂæàÂ§öÁöÑÁ¨îÁîªÔºåÁ±ª‰ºº‰∫é‰∏Ä‰∏™Ëã±ÊñáÂçïËØçÂåÖÂê´ÂæàÂ§öÁöÑÊãâ‰∏ÅÂ≠óÊØçÔºåÂú®Ëøô‰∏™Âü∫Á°Ä‰πã‰∏äÔºåÊèêÂá∫‰∫ÜÁ¨îÁîªÁöÑn-gramÁâπÂæÅ„ÄÇËøô‰∏™ÊÄùÊÉ≥Êù•Ê∫ê‰∫é2016Âπ¥facebookÊèêÂá∫ÁöÑËÆ∫ÊñáÔºàEnriching Word Vectors with Subword InformationÔºâÔºåcw2vecÂèØ‰ª•Áß∞‰πã‰∏∫‰∏≠ÊñáÁâàÊú¨ÁöÑfasttextÔºåÂà©Áî®Á¨îÁîªÁöÑn-gram‰ª£skip-gram‰∏≠ÁöÑËØçËØ≠ËøõË°åËÆ≠ÁªÉ„ÄÇ

15. „ÄäRadical Enhanced Chinese Word Embedding„Äã
ÊòØÂü∫‰∫éCBOWÊù•ËøõË°åÁöÑÊîπËøõÔºåÈÄöËøáRadicalÔºàÈÉ®È¶ñÔºâÊù•Â¢ûÂº∫word embeddingÔºåÁß∞‰πã‰∏∫RECWEÊ®°Âûã„ÄÇ‰∏∫‰∫ÜËÉΩÂ§üÂÖÖÂàÜÁöÑÊåñÊéòÂÜÖÈÉ®ËØ≠‰πâ‰ø°ÊÅØÔºåËøòÂØπradicalËøõË°å‰∫ÜËΩ¨Êç¢Â§ÑÁêÜÔºöÂ¶ÇÂçï‰∫∫ÊóÅ-->‰∫∫

16. [Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf), Yonghui Wu, Mike Schuster, et. al., 2016.
To improve handling of rare words, we divide words into a limited set of common sub-word units (‚Äúwordpieces‚Äù) for both input and output. This method provides a good balance between the flexibility of ‚Äúcharacter‚Äù-delimited models and the efficiency of ‚Äúword‚Äù-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system.

17. [Character and Subword-Based word Representation for Neural Language Modeling prediction. Matthieu Labeau, Alexandre Allauzen. ACL 2017](http://aclweb.org/anthology/W17-41)

## CO-AUTHORS

| Co-Author | GitHub | Bio & Research Direction |
| --- | --- | --- |
| Huan LI | @zixia | BUPT CS Ph.D. for Conversational AI |
| Shirui WANG | @wangshirui33 | BUPT CS Ph.D. for Language Embedding |

## COPYRIGHT & LICENSE

* Code & Docs ¬© 2016-2018 Huan LI \<zixia@zixia.net\>
* Code released under the Apache-2.0 License
* Docs released under Creative Commons CC-BY 4.0
* Even if enclosed inside a notebook or served as part of the interactive article.

